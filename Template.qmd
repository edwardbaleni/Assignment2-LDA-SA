---
title: "Template"
author: "Edward Baleni, Andrea Plumbley, Luke Barnes"
format: html
editor: visual
---

## Abstract

## Introduction

The aim of this assignment is to provide a descriptive analysis of the State of the Nation speeches in South Africa between 1994 and 2022. This descriptive analysis is performed using sentiment analysis and topic modelling. Sentiment analysis aims to describe the content of text in terms of its 'emotions' (REFERENCE). The purpose of performing sentiment analysis on these speeches is to identify the overall tone or emotion of the speech and identify how this might change over time or as presidents change. Topic modelling is a technique that aims to summarize text, in this case speeches, in terms of a number of topics. It is a way of categorizing speeches into different topics and identifying the main themes of a body of text (REFERENCE). This is of interest in order to identify what topics or themes are important in the context of the government of South Africa and how these themes or main topics may change over time or over different presidents.

Before the analysis is applied to the data a brief literature review will be given. Following this the data and any cleaning that was done will be discussed. The methods for performing sentiment analysis and topic modelling will be outlined. Finally the results of the analysis on the SONA speeches data will be presented and discussed.

Chat GPT was used as an aid to this assignment. The purpose of this was to experiment large language models and to assess Chat GPT's ability to assist with the assignment. A brief summary of how well it did and how it responded to different prompts and guides in given in Appendix A (which can be found on Appendix A tab of this website).

## Literature Review

#### ***Sentiment Analysis***

#### ***Topic Modelling***

## Data

As mentioned, the data for this analysis is a collection of 36 speeches delivered as State of the Nation Addresses between 1994 and 2022. These speeches were delivered by 6 different presidents: de Klerk, Mandela, Mbeki, Motlanthe, Zuma and Ramaphosa.

This data needed to be cleaned before sentiment analysis or topic modelling could be done. The data was cleaned by removing punctuation marks and numbers from the speeches. Stop words, which are common words such as 'the', 'and' and 'they' were also removed from the speeches.

In order to perform sentiment analysis the speeches needed to be tokenized into shorter parts such as bigrams and words in order to assess the sentiment of each smaller part and then aggregate these sentiments to understand the sentiment of the speech as a whole. The speeches are tokenized first into sentences and then into words using the unnest_tokens() function in R.

-   Explain why bigrams were necessary and how this worked - Were stop words not removed?? Because we need not and no etc

How was data structured for topic modelling??

## Methods

### *Sentiment Analysis*

In order to perform sentiment analysis, sentiment lexicons are used. These lexicons contain many words which are labelled according to their sentiment. The main lexicon that was made use of here for analysis was the *bing* dictionary which labels words as positive or negative. The *affin* dictionary gives each word a score between negative five and five, from negative to positive sentiments.

In order to get each words sentiment, the data frame containing individual words, with an associated president, year ans sentence ID is left joined with the *bing* lexicon in order to get their associated positive or negative sentiment. Because many of the words in the speeches are not in the dictionary, these words are assigned with a sentiment value of neutral instead of removing them from analysis. This is not an ideal situation as it may mean that words which are very emotively loaded are being assigned as neutral and so one could miss the true sentiment of a sentence or speech however it is better than removing all these words from the analysis completely.

In order to analyze the sentiment of different presidents we can consider each presidents most commonly used positive and negative words which can be found using filtering by president and filtering by positive or negative sentiment.

In addition one can consider how sentiment changes over time and by president by considering the number of positive and negative sentiment associated words and seeing how this changes over time. This can also be done using the filter() function and filtering by year. In this way one can see if the percentage of positive sentiment in a speech increased or decreased as well as identify the net sentiment of the SONA speeches over time.

-   NEED TO ADD BIGRAMS SECTION OF SENTIMENT ANALYSIS

### *Topic Modelling*

Add topic modelling methods and explain LDA.

## Results and Discussion 

Having outlined the methods used to perform sentiment analysis and topic modelling, the results and analysis of the speeches is now given.

### *Sentiment Analysis*

```{r, echo = FALSE}
### Libraries

library(tidyverse)
library(tidytext)
library(tokenizers)
library(gghighlight)
library(tictoc)
library(ggpubr)

### Load Data

load("SonaData.RData")
load("dsfi-lexicons.Rdata")

### Separate speeches into sentences and sentences into words

unnest_reg = "[^\\w_#@']"

speechSentences = as_tibble(sona) %>%
  rename(president = president_13) %>%
  unnest_tokens(sentences, speech, token = "sentences") %>%
  select(president, year, sentences) %>%
  mutate(sentences, sentences = str_replace_all(sentences, "â€™", "'")) %>%
  mutate(sentences, sentences = str_replace_all(sentences, "'", "")) %>%
  mutate(sentences, sentences = str_remove_all(sentences, "[0-9]")) %>%
  mutate(sentID = row_number())

wordsWithSentID = speechSentences %>% 
  unnest_tokens(word, sentences, token = 'regex', pattern = unnest_reg) %>%
  filter(str_detect(word, '[a-z]')) %>%
  filter(!word %in% stop_words$word) %>%
  select(sentID, president, year, word)

### Join with Sentiment Lexicon

wordsSentiment = wordsWithSentID %>% 
  left_join(bing, by = "word") %>%
  rename(bing_sentiment = sentiment) %>%
  mutate(bing_sentiment = ifelse(is.na(bing_sentiment), "neutral", bing_sentiment))

#head(wordsSentiment)
table(wordsSentiment$bing_sentiment)

### Most frequent Words
## Positive

## How often each president has said the top 10 most frequent positive words

wordsSentiment %>%
  filter(bing_sentiment == "positive") %>%
  count(president, word) %>%
  group_by(president) %>% 
  filter(rank(desc(n)) <= 10) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col() + 
  facet_wrap(~president) + coord_flip() + xlab(" ")

```

```{r}
## Each Presidents Top 15 Most Frequent Positive words
## Top 5 is highlighted

P1 = wordsSentiment %>%
  filter(president == "Mandela") %>%
  filter(bing_sentiment == "positive") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "purple", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 6)

P2 = wordsSentiment %>%
  filter(president == "Mbeki") %>%
  filter(bing_sentiment == "positive") %>%
  count(word) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "purple", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P3 = wordsSentiment %>%
  filter(president == "Motlanthe") %>%
  filter(bing_sentiment == "positive") %>%
  count(word) %>%
  #filter(id <= 15) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "purple", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speech") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P4 = wordsSentiment %>%
  filter(president == "Ramaphosa") %>%
  filter(bing_sentiment == "positive") %>%
  count(word) %>%
  #filter(id <= 15) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "purple", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P5 = wordsSentiment %>%
  filter(president == "Zuma") %>%
  filter(bing_sentiment == "positive") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "purple", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P6 = wordsSentiment %>%
  filter(president == "deKlerk") %>%
  filter(bing_sentiment == "positive") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "purple", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speech") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

ggarrange(P1, P2, P3, P4, P5, P6, ncol=3, nrow=2)
```

```{r, echo = FALSE}
## Each Presidents Most Frequent Negative words
## Top 5 is highlighted

P1 = wordsSentiment %>%
  filter(president == "Mandela") %>%
  filter(bing_sentiment == "negative") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "orange", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P2 = wordsSentiment %>%
  filter(president == "Mbeki") %>%
  filter(bing_sentiment == "negative") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "orange", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P3 = wordsSentiment %>%
  filter(president == "Motlanthe") %>%
  filter(bing_sentiment == "negative") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "orange", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speech") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P4 = wordsSentiment %>%
  filter(president == "Ramaphosa") %>%
  filter(bing_sentiment == "negative") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "orange", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P5 = wordsSentiment %>%
  filter(president == "Zuma") %>%
  filter(bing_sentiment == "negative") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "orange", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speeches") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

P6 = wordsSentiment %>%
  filter(president == "deKlerk") %>%
  filter(bing_sentiment == "negative") %>%
  count(word) %>%
  #filter(rank(desc(n)) <= 20) %>%
  arrange(desc(n)) %>%
  mutate(id = 1:dim(.)[1]) %>%
  filter(id <= 15) %>%
  ggplot(aes(reorder(word, n), n)) + geom_col(fill = "orange", col = "black") + 
  coord_flip() + 
  xlab(" ") + ylab("Times Used in Speech") +
  theme_bw(base_size = 12) +
  gghighlight(id <= 5)

ggarrange(P1, P2, P3, P4, P5, P6, ncol=3, nrow=2)

### Count the positive and negative sentiments in each speech 

wordsSentiment %>%
  group_by(year, president) %>%
  filter(bing_sentiment == "positive") %>%
  count(bing_sentiment) %>%
  ggplot(aes(x = year, y = n, shape = president)) + 
  geom_point(col = "purple", size = 5, stroke = 2) +
  xlab("Year") + ylab("Number of Positive Sentiments in Speech") +
  theme_bw(base_size = 12) + 
  scale_x_discrete(name = "Year", 
                   breaks = c("1994","1999","2004", "2009", 
                              "2014", "2019", "2023")) +
  scale_shape_manual(values = c(5, 15, 1, 18, 0, 16))

wordsSentiment %>%
  group_by(year, president) %>%
  filter(bing_sentiment == "negative") %>%
  count(bing_sentiment) %>%
  ggplot(aes(x = year, y = n, shape = president)) + 
  geom_point(col = "orange", size = 5, stroke = 2) +
  xlab("Year") + ylab("Number of Negative Sentiments in Speech") + 
  theme_bw(base_size = 12) + 
  scale_x_discrete(name = "Year", 
                   breaks = c("1994","1999","2004", "2009", 
                              "2014", "2019", "2023")) +
  scale_shape_manual(values = c(5, 15, 1, 18, 0, 16)) 

## Net Sentiment of Speech

wordsSentiment %>%
  group_by(year, president, bing_sentiment) %>%
  filter(bing_sentiment == "negative" | bing_sentiment == "positive") %>%
  count(bing_sentiment) %>%
  ungroup(bing_sentiment) %>%
  mutate(netSent = n - first(n)) %>%
  filter(bing_sentiment == "positive") %>%
  ggplot(aes(x = year, y = netSent, shape = president)) + 
  geom_point(col = "red", size = 5, stroke = 2) +
  xlab("Year") + ylab("Number of Net Positive Sentiments in Speech") + 
  theme_bw(base_size = 12) + 
  scale_x_discrete(name = "Year", 
                   breaks = c("1994","1999","2004", "2009", 
                              "2014", "2019", "2023")) +
  scale_shape_manual(values = c(5, 15, 1, 18, 0, 16)) 

## Change in Net Positive Sentiment between first and last speech

wordsSentiment %>%
  group_by(year, president, bing_sentiment) %>%
  filter(bing_sentiment == "negative" | bing_sentiment == "positive") %>%
  count(bing_sentiment) %>%
  ungroup(bing_sentiment) %>%
  mutate(netSent = n - first(n)) %>%
  filter(bing_sentiment == "positive") %>%
  ungroup(year) %>%
  mutate(changeNetSent = last(netSent) - netSent) %>% 
  filter(row_number() == 1) %>%
  ggplot(aes(x = as.factor(president), y = changeNetSent)) + 
  geom_bar(fill = "red", stat = "identity") +
  xlab("President") + ylab("Change in Net Positive Sentiments in Speech") + 
  theme_bw(base_size = 12) 
```

## 

## Conclusion
